{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lesson 3\n",
    "\n",
    "\n",
    "Things we will learn in this lesson:\n",
    "\n",
    "    - forward pass caluclation using hand written code\n",
    "    - forward pass using inbuilt method Linear()\n",
    "    - pytorch custom module creation for simple linear regression\n",
    "    - optimization algorithm -Batch Gradient descent\n",
    "    - Stochastic Gradient Descent\n",
    "    - Mini- Batch gradient Descent\n",
    "    - Simple Linear regrssion using inbuilt pytorch loss and optim functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass method\n",
    "**Lets  define a function by our self say y = wx+b **\n",
    "\n",
    "where, x is the input variable \n",
    "       w is the weight/slope \n",
    "       b is the bias/y-intercept\n",
    "        \n",
    "**Now lets calculate the value of y for x = 1.0, w=2.0 and b=-1.0 using torch functions as practise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(weight, bias, input_x):\n",
    "    y = bias + (weight*input_x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of yhat is: tensor([1.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Weight & bias tensors are initialized \n",
    "#(requires_grad is set True for variable which are to be learned/updated during model building)\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(-1.0, requires_grad=True)\n",
    "\n",
    "x= torch.tensor([1.0])\n",
    "yhat = forward(w, b, x)\n",
    "\n",
    "\n",
    "print('The value of yhat is:', yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets do the same for a set of input rows sent to the \"Foraward method\"**\n",
    "\n",
    "This would result in a linear function applied to every row of the input and the outputs are calculated by boradcasting the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of yhat with input x: tensor([[1],\n",
      "        [5]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1],[3]])\n",
    "\n",
    "yhat = forward(w, b, x)\n",
    "\n",
    "print('The output of yhat with input x:', yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear() inbuilt method for Forward calculation\n",
    "### PYTORCH has made our lives easy by providng all these linear caluclations handy through the 'class Linear' in package torch.nn\n",
    "\n",
    "**Lets make use of Linear method from torch.nn to calculate the value of an output for a randomly set weight and bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = Linear(in_features=1, out_features=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *'model' object of 'Linear' class shows,* **\n",
    "\n",
    "the weight/slope as first parameter 0.7645\n",
    "the bisas as second parameter 0.8300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.7645]], requires_grad=True), Parameter containing:\n",
       " tensor([0.8300], requires_grad=True)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python dictionary:  OrderedDict([('weight', tensor([[0.7645]])), ('bias', tensor([0.8300]))])\n",
      "keys:  odict_keys(['weight', 'bias'])\n",
      "values:  odict_values([tensor([[0.7645]]), tensor([0.8300])])\n"
     ]
    }
   ],
   "source": [
    "print(\"Python dictionary: \",model.state_dict())\n",
    "print(\"keys: \",model.state_dict().keys())\n",
    "print(\"values: \",model.state_dict().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the object 'model' of class linear, avoids us to write a seperate method called forward and explicitly send their paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of yhat is: tensor([3.1236], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0])\n",
    "\n",
    "yhat = model(x)\n",
    "print('The value of yhat is:', yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Module for Linear Regression using Linear()\n",
    "#### Now we are comfortable using the  in-built pytorch 'Linear' class for performing the forward pass calculation.\n",
    "\n",
    "Lets move ahead and build a custom module for Linear regression as a whole (leveraging the in-built features like 'torch.nn.Linear')\n",
    "\n",
    "**For building customer module we will use \"torch.nn.Module\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear_Regression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Linear_Regression, self).__init__()\n",
    "        self.Linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward_pass(self, input_x):\n",
    "        return self.Linear(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python dictionary:  OrderedDict([('Linear.weight', tensor([[-0.2343]])), ('Linear.bias', tensor([0.9186]))])\n",
      "keys:  odict_keys(['Linear.weight', 'Linear.bias'])\n",
      "values:  odict_values([tensor([[-0.2343]]), tensor([0.9186])])\n"
     ]
    }
   ],
   "source": [
    "#creating object 'model' for Linear_Regression class\n",
    "model = Linear_Regression(1,1)\n",
    "\n",
    "#State_dict is the default weights and bias initialized by the 'Linear' class\n",
    "print(\"Python dictionary: \",model.state_dict())\n",
    "print(\"keys: \",model.state_dict().keys())\n",
    "print(\"values: \",model.state_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of yhat is: tensor([[ 0.6843],\n",
      "        [-0.2528]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.], [5.]])\n",
    "\n",
    "yhat = model.forward_pass(x)\n",
    "print('The value of yhat is:', yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch way of building simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first define the forward pass function and MSE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss_mse(y,yhat):\n",
    "    return torch.mean(y - yhat)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial value of weight is set to -10.0 and the input vector is also created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(-10.0, requires_grad=True)\n",
    "\n",
    "# create a range of input value with .arange() function where 0.1 is the stepsize from -3 to 3(resulting in 60 elements). \n",
    "# View function creates transposes matrix from (1x60) to (60x1)\n",
    "x = torch.arange(-3,3,0.1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2430d71e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets create the actual output y\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = f + (0.1*torch.rand(x.size()))\n",
    "\n",
    "plt.plot(x.numpy(),y.numpy(),'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code iterates for 4 epochs,\n",
    "\n",
    "i.e 1 epoch is equal to sending all input of x and calulating forward and backward pass(1 epoch = 1 update of weight w)\n",
    "\n",
    "In each epoch, loss is calculated for 'w' weught output for every input of x using MSE. Partial diff is done using \".backward()\"\n",
    "\n",
    "weight 'w' is updated in successive steps with account to lr. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.9970)\n",
      "tensor(-9.9940)\n",
      "tensor(-9.9910)\n",
      "tensor(-9.9881)\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "for epoch in range(4):\n",
    "    yhat = forward(x)\n",
    "    loss = loss_mse(y,yhat)\n",
    "    loss.backward()\n",
    "    w.data = w.data - (0.1*w.grad.data)\n",
    "    print(w.data)\n",
    "    cost.append(loss.item())\n",
    "    w.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08915474265813828,\n",
       " 0.08906543999910355,\n",
       " 0.08897723257541656,\n",
       " 0.08888784050941467]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method is called \"BATCH GRADIENT DESCENT\" since all the examples in the dataset is taken at once to calculate cost and backpropagate to \n",
    "optimize the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastitc Gradient Descent\n",
    "\n",
    "This method of optimizarion takes every individual sample and caluclates loss and then optimize it by partial differentiation\n",
    "\n",
    "Hence the total number of iteration per epoch = total number of samples = total number of updates in weight w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = torch.tensor(-10.0, requires_grad=True)\n",
    "\n",
    "# create a range of input value with .arange() function where 0.1 is the stepsize from -3 to 3(resulting in 60 elements). \n",
    "# View function creates transposes matrix from (1x60) to (60x1)\n",
    "x = torch.arange(-3,3,0.1).view(-1,1)\n",
    "\n",
    "#Lets create the actual output y\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = f + (0.1*torch.rand(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weight for epoch 0 is -9.821308135986328 with total loss of 5.322175860404968\n",
      "Updated weight for epoch 1 is -9.647895812988281 with total loss of 5.01216085255146\n",
      "Updated weight for epoch 2 is -9.47961139678955 with total loss of 4.720197334885597\n",
      "Updated weight for epoch 3 is -9.316304206848145 with total loss of 4.445249117910862\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "total_loss = []\n",
    "for epoch in range(4):\n",
    "    total = 0\n",
    "    for i in zip(x,y):\n",
    "        yhat = forward(x)\n",
    "        loss = loss_mse(y,yhat)\n",
    "        loss.backward()\n",
    "        w.data = w.data - (0.1*w.grad.data)\n",
    "        #print(w.data)\n",
    "        cost.append(loss.item())\n",
    "        w.grad.data.zero_()\n",
    "        total = total + loss.item() #total loss for the iteration\n",
    "    total_loss.append # total loss for the epoch   \n",
    "    print('Updated weight for epoch {} is {} with total loss of {}'.format(epoch, w.data, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini -Batch Gradient Descent\n",
    "\n",
    "This method is similar to SGD but takes in bunches instaed of individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = torch.tensor(-10.0, requires_grad=True)\n",
    "\n",
    "# create a range of input value with .arange() function where 0.1 is the stepsize from -3 to 3(resulting in 60 elements). \n",
    "# View function creates transposes matrix from (1x60) to (60x1)\n",
    "x = torch.arange(-3,3,0.1).view(-1,1)\n",
    "\n",
    "#Lets create the actual output y\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = f + (0.1*torch.rand(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weight for epoch 0 is -2.91878604888916 with total loss of 581.586856842041\n",
      "Updated weight for epoch 1 is -3.017382860183716 with total loss of 0.07656095642596483\n",
      "Updated weight for epoch 2 is -3.016010046005249 with total loss of 0.008780610223766416\n",
      "Updated weight for epoch 3 is -3.016029119491577 with total loss of 0.008141433470882475\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "total_loss = []\n",
    "bs = 3\n",
    "for epoch in range(4):\n",
    "    total = 0\n",
    "    start = 0\n",
    "    batch_size = bs\n",
    "    for i in range(batch_size):\n",
    "        x_ = x[start:batch_size]\n",
    "        y_ = y[start:batch_size]\n",
    "        yhat = forward(x_)\n",
    "        loss = loss_mse(y_,yhat)\n",
    "        loss.backward()\n",
    "        w.data = w.data - (0.1*w.grad.data)\n",
    "        #print(w.data)\n",
    "        cost.append(loss.item())\n",
    "        w.grad.data.zero_()\n",
    "        total = total + loss.item() #total loss for the iteration\n",
    "        start = batch_size\n",
    "        batch_size = batch_size + bs\n",
    "    total_loss.append # total loss for the epoch   \n",
    "    print('Updated weight for epoch {} is {} with total loss of {}'.format(epoch, w.data, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim function to automate optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('Linear.weight', tensor([[-0.2751]])), ('Linear.bias', tensor([-0.3833]))])\n"
     ]
    }
   ],
   "source": [
    "model = Linear_Regression(1,1)\n",
    "print(model.state_dict())\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a range of input value with .arange() function where 0.1 is the stepsize from -3 to 3(resulting in 60 elements). \n",
    "# View function creates transposes matrix from (1x60) to (60x1)\n",
    "x = torch.arange(-3,3,0.1).view(-1,1)\n",
    "\n",
    "#Lets create the actual output y\n",
    "\n",
    "f = -3*x\n",
    "\n",
    "y = f + (0.1*torch.rand(x.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total loss for epoch 0 is 1617.740312576294 with parameters {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2487013126288, 2487013128592]}]}\n",
      "The total loss for epoch 1 is 1617.7398281097412 with parameters {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2487013126288, 2487013128592]}]}\n",
      "The total loss for epoch 2 is 1617.7398204803467 with parameters {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2487013126288, 2487013128592]}]}\n",
      "The total loss for epoch 3 is 1617.7398052215576 with parameters {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2487013126288, 2487013128592]}]}\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "total_loss = []\n",
    "for epoch in range(4):\n",
    "    total = 0\n",
    "    for i in zip(x,y):\n",
    "        yhat = model.forward_pass(i[0])\n",
    "        loss = loss_mse(y,yhat)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        total = total + loss.item()\n",
    "        optimizer.step() #this method update the weights w based on diff values from loss.backward() under the hood\n",
    "    print(\"The total loss for epoch {} is {} with parameters {}\".format(epoch,total,optimizer.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus using nn module to select the required buit-in loss function and then optim function to select required optimization algorithm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
